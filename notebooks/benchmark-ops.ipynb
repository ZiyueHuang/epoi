{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a120bbad",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "This notebook shows the benchmark results of all covered ops.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de4e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def build_package(name, url, commit=None, deps=True):\n",
    "    import importlib\n",
    "    import os, sys\n",
    "    if importlib.util.find_spec(name) is None:\n",
    "        os.system(f\"git clone {url} {name} || true\")\n",
    "        if commit is not None:\n",
    "            os.system(f\"cd {name}; git checkout {commit}\")\n",
    "        os.system(f\"cd {name}; git submodule update --init --recursive\")\n",
    "        no_deps = \"\"\n",
    "        if deps:\n",
    "            os.system(f\"cd {name}; pip3 install -r requirements.txt || true\")\n",
    "        else:\n",
    "            no_deps = \"--no-deps\"\n",
    "        clear_output()\n",
    "        os.system(f'cd {name}; pip3 install -e \".[dev]\" {no_deps}')\n",
    "\n",
    "build_package(\"transformers\", \"https://github.com/huggingface/transformers.git\", deps=False)\n",
    "build_package(\"xformers\", \"https://github.com/facebookresearch/xformers.git\")\n",
    "build_package(\"epoi\", \"https://github.com/comaniac/epoi.git\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606041c3",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df76cc65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoscrolling long output is disabled\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Javascript\n",
    "\n",
    "disable_js = \"\"\"\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def load_ipython_extension():\n",
    "    display(Javascript(disable_js))\n",
    "    print (\"autoscrolling long output is disabled\")\n",
    "    \n",
    "load_ipython_extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd41d37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Environment =====\n",
      "\n",
      "GPU: Tesla V100-SXM2-16GB\n",
      "\n",
      "PyTorch Configuration\n",
      "   Config         Value\n",
      "-------------  ------------\n",
      "   Version     1.13.0+cu117\n",
      "Built w. CUDA      11.7\n",
      "\n",
      "\n",
      "Other Libraries Configuration\n",
      "  Package       Version                   Commit SHA\n",
      "------------  -----------  ----------------------------------------\n",
      "    epoi        0.1.dev    c1a64b8a13542174dfe74dad628ea26ce57d7fc1\n",
      "transformers  4.24.0.dev0  12ce2941c7b67c0dedac0f0468b3ed854fa940ab\n",
      "  xformers    0.0.14.dev   ba93c5012d00bd1b010514a7bc9bd938c1ad6149\n",
      "   triton        2.0.0                       N/A\n",
      "    apex          0.1                        N/A\n",
      "===== Environment =====\n",
      "\n",
      "[2022-10-28 19:54:50] INFO main: Selected bias_gelu\n",
      "[2022-10-28 19:54:50] INFO main: Selected dropout_add_ln\n",
      "[2022-10-28 19:54:50] INFO main: Selected bert_attention\n",
      "[2022-10-28 19:54:50] INFO main: Selected gpt_attention\n",
      "[2022-10-28 19:54:50] INFO main: Selected qkv_self_attn\n",
      "[2022-10-28 19:54:50] INFO main: Selected layer_norm\n",
      "[2022-10-28 19:54:50] INFO main: Selected softmax\n",
      "[2022-10-28 19:54:50] INFO main: Running selected 7/7 cases\n",
      "[2022-10-28 19:54:50] INFO main: [1/7] Benchmarking bias_gelu\n",
      "[------------------------------------------- Bias+GeLU --------------------------------------------]\n",
      "                        |  Eager (FP32)  |  TS+nvFuser (FP32)  |  Eager (FP16)  |  TS+nvFuser (FP16)\n",
      "1 threads: -----------------------------------------------------------------------------------------\n",
      "      (8, 512, 1024)    |      230.2     |         328.1       |      205.7     |         341.8     \n",
      "      (8, 512, 768)     |      200.9     |         330.2       |      205.3     |         341.1     \n",
      "      (2, 1024, 4096)   |      397.6     |         397.4       |      239.6     |         349.0     \n",
      "      (16, 512, 8192)   |     2628.8     |        2043.1       |     1415.3     |        1093.8     \n",
      "      (16, 512, 32768)  |    10271.8     |        7831.3       |     5450.5     |        3975.0     \n",
      "      (4, 2048, 8192)   |     2635.2     |        2038.3       |     1416.9     |        1092.8     \n",
      "      (4, 2048, 32768)  |    10275.3     |        7838.0       |     5444.1     |        3974.5     \n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "     Shape         Eager (FP32)    TS+nvFuser (FP32)    Eager (FP16)    TS+nvFuser (FP16)\n",
      "----------------  --------------  -------------------  --------------  -------------------\n",
      " (8, 512, 1024)      64.0044            64.0044           48.0024            48.0024\n",
      " (8, 512, 768)       48.0034            48.0034            36.002            36.002\n",
      "(2, 1024, 4096)      128.016            128.016           96.0083            96.0083\n",
      "(16, 512, 8192)        768              592.032             384              336.016\n",
      "(16, 512, 32768)       3072             2176.13             1536             1152.06\n",
      "(4, 2048, 8192)        768              592.032             384              336.016\n",
      "(4, 2048, 32768)       3072             2176.13             1536             1152.06\n",
      "\n",
      "Memory is in MBs and excludes inputs/outputs.\n",
      "\n",
      "[2022-10-28 19:55:32] INFO main: [2/7] Benchmarking dropout_add_ln\n",
      "[------------------------------------- Dropout+Add+LayerNorm --------------------------------------]\n",
      "                        |  Eager (FP32)  |  TS+nvFuser (FP32)  |  Eager (FP16)  |  TS+nvFuser (FP16)\n",
      "1 threads: -----------------------------------------------------------------------------------------\n",
      "      (32, 128, 768)    |      373.1     |         628.2       |      311.0     |         665.6     \n",
      "      (4, 512, 768)     |      307.6     |         633.3       |      307.4     |         664.6     \n",
      "      (8, 512, 1024)    |      472.8     |         685.7       |      314.3     |         677.5     \n",
      "      (64, 128, 1024)   |      834.9     |         790.5       |      478.1     |         688.4     \n",
      "      (16, 512, 8192)   |     6842.3     |        4177.2       |     3573.7     |        2300.8     \n",
      "      (16, 512, 32768)  |    27444.5     |       22598.1       |    14509.6     |       10705.4     \n",
      "      (4, 2048, 8192)   |     6638.7     |        4195.4       |     3572.6     |        2320.5     \n",
      "      (4, 2048, 32768)  |    27658.6     |       22538.9       |    14308.5     |       10728.1     \n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "     Shape         Eager (FP32)    TS+nvFuser (FP32)    Eager (FP16)    TS+nvFuser (FP16)\n",
      "----------------  --------------  -------------------  --------------  -------------------\n",
      " (32, 128, 768)         51              63.0371              27              33.0342\n",
      " (4, 512, 768)          26              32.0215              14              17.0186\n",
      " (8, 512, 1024)         68              84.0391              36              44.0352\n",
      "(64, 128, 1024)        136              168.07               72              88.0664\n",
      "(16, 512, 8192)        1088             1344.12             576              704.094\n",
      "(16, 512, 32768)       4352             6400.34             2304             3328.22\n",
      "(4, 2048, 8192)        1088             1344.12             576              704.094\n",
      "(4, 2048, 32768)       4352             6400.34             2304             3328.22\n",
      "\n",
      "Memory is in MBs and excludes inputs/outputs.\n",
      "\n",
      "[2022-10-28 19:57:46] INFO main: [3/7] Benchmarking bert_attention\n",
      "[2022-10-28 19:57:49] WARNING bencher: Skip correctness checking for xFormers vanilla FlashAttn: Forward failed\n",
      "[2022-10-28 19:57:49] INFO bencher: Correctness checking for xFormers cutlass FlashAttn is passed\n",
      "[2022-10-28 19:57:50] WARNING bencher: Skip correctness checking for xFormers triton FlashAttn: Forward failed\n",
      "[--------- Bert Attention (Attn) and FlashAttention (FA) without mask --------]\n",
      "                                         |  HF (Attn)  |  xFormers cutlass (FA)\n",
      "1 threads: --------------------------------------------------------------------\n",
      "      (8, 512, 1024, 16, 4096, 30522)    |      4.3    |            2.8        \n",
      "      (16, 512, 8192, 64, 32768, 50264)  |    134.7    |          129.5        \n",
      "      (4, 2048, 8192, 64, 32768, 50264)  |    198.3    |          195.7        \n",
      "\n",
      "Times are in milliseconds (ms).\n",
      "\n",
      "              Shape                 HF (Attn)    xFormers cutlass (FA)\n",
      "---------------------------------  -----------  -----------------------\n",
      " (8, 512, 1024, 16, 4096, 30522)       288               96.25\n",
      "(16, 512, 8192, 64, 32768, 50264)     2816               1026\n",
      "(4, 2048, 8192, 64, 32768, 50264)     8704               2562\n",
      "\n",
      "Memory is in MBs and excludes inputs/outputs.\n",
      "\n",
      "[2022-10-28 20:03:41] INFO main: [4/7] Benchmarking gpt_attention\n",
      "[2022-10-28 20:03:42] INFO bencher: Correctness checking for xFormers FlashAttn (cutlass) is passed\n",
      "[2022-10-28 20:03:43] WARNING bencher: Skip correctness checking for xFormers FlashAttn (triton): Forward failed\n",
      "[----- GPT Attention (Attn) and FlashAttention (FA) without mask ------]\n",
      "                                  |  HF (Attn)  |  xFormers cutlass (FA)\n",
      "1 threads: -------------------------------------------------------------\n",
      "      (8, 1024, 1024, 16, 50257)  |     14.3    |            6.1        \n",
      "      (16, 512, 8192, 64, 50264)  |    184.2    |          164.6        \n",
      "      (4, 2048, 8192, 64, 50264)  |    254.2    |          198.1        \n",
      "\n",
      "Times are in milliseconds (ms).\n",
      "\n",
      "          Shape              HF (Attn)    xFormers cutlass (FA)\n",
      "--------------------------  -----------  -----------------------\n",
      "(8, 1024, 1024, 16, 50257)     1091              178.502\n",
      "(16, 512, 8192, 64, 50264)    2688.27            1284.02\n",
      "(4, 2048, 8192, 64, 50264)    8836.02            1284.02\n",
      "\n",
      "Memory is in MBs and excludes inputs/outputs.\n",
      "\n",
      "[2022-10-28 20:11:01] INFO main: [5/7] Benchmarking qkv_self_attn\n",
      "[----------------- QKV in Self-Attention ------------------]\n",
      "                           |  NoFuse (FP16)  |  Fused (FP16)\n",
      "1 threads: -------------------------------------------------\n",
      "      (4, 512, 1024, 16)   |       1181.5    |       850.3  \n",
      "      (8, 512, 1024, 16)   |       1361.0    |      1414.6  \n",
      "      (16, 512, 1024, 16)  |       2313.6    |      2646.8  \n",
      "      (16, 512, 8192, 64)  |     110609.9    |    117115.9  \n",
      "      (4, 2048, 8192, 64)  |     110482.7    |    117106.6  \n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "       Shape          NoFuse (FP16)    Fused (FP16)\n",
      "-------------------  ---------------  --------------\n",
      "(4, 512, 1024, 16)       42.0063         82.0063\n",
      "(8, 512, 1024, 16)       78.0063         144.006\n",
      "(16, 512, 1024, 16)      150.006         200.006\n",
      "(16, 512, 8192, 64)      1152.05         1376.05\n",
      "(4, 2048, 8192, 64)      1152.05         1376.05\n",
      "\n",
      "Memory is in MBs and excludes inputs/outputs.\n",
      "\n",
      "[2022-10-28 20:15:06] INFO main: [6/7] Benchmarking layer_norm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-10-28 20:15:06] INFO bencher: Correctness checking for Apex (FP16) is passed\n",
      "[2022-10-28 20:15:07] WARNING bencher: Skip correctness checking for Triton (FP16): Forward failed\n",
      "[2022-10-28 20:15:07] INFO bencher: Correctness checking for xFormers (FP16) is passed\n",
      "[2022-10-28 20:15:08] WARNING bencher: Skip Triton (FP16): Forward failed with shape (32, 128, 768)\n",
      "[2022-10-28 20:15:10] WARNING bencher: Skip Triton (FP16): Forward failed with shape (8, 512, 1024)\n",
      "[2022-10-28 20:15:18] WARNING bencher: Skip Triton (FP16): Forward failed with shape (16, 512, 8192)\n",
      "[2022-10-28 20:15:27] WARNING bencher: Skip Triton (FP16): Forward failed with shape (4, 2048, 8192)\n",
      "[------------------------------------------------- LayerNorm -------------------------------------------------]\n",
      "                       |  PyTorch (FP32)  |  Apex (FP32)  |  PyTorch (FP16)  |  Apex (FP16)  |  xFormers (FP16)\n",
      "1 threads: ----------------------------------------------------------------------------------------------------\n",
      "      (32, 128, 768)   |       260.7      |      296.9    |       191.9      |      250.6    |        884.9    \n",
      "      (8, 512, 1024)   |       307.3      |      340.4    |       210.7      |      256.4    |        877.8    \n",
      "      (16, 512, 8192)  |      4614.8      |     4353.1    |      2355.6      |     2546.8    |       1696.6    \n",
      "      (4, 2048, 8192)  |      4511.2      |     4357.1    |      2348.6      |     2530.9    |       1693.2    \n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "     Shape        PyTorch (FP32)    Apex (FP32)    PyTorch (FP16)    Apex (FP16)    xFormers (FP16)\n",
      "---------------  ----------------  -------------  ----------------  -------------  -----------------\n",
      "(32, 128, 768)       24.0371          24.1309         12.0342          12.1279          12.7861\n",
      "(8, 512, 1024)       32.0391          32.1641         16.0352          16.1602          17.0371\n",
      "(16, 512, 8192)      512.125          513.125         256.094          257.094          258.094\n",
      "(4, 2048, 8192)      512.125          513.125         256.094          257.094          258.094\n",
      "\n",
      "Memory is in MBs and excludes inputs/outputs.\n",
      "\n",
      "[2022-10-28 20:15:28] INFO main: [7/7] Benchmarking softmax\n",
      "[2022-10-28 20:15:28] WARNING bencher: Correctness checking for Megatron-LM (Comp-FP32) failed at initialization\n",
      "[2022-10-28 20:15:34] INFO bencher: Correctness checking for xFormers (Comp-FP32) is passed\n",
      "[2022-10-28 20:15:35] WARNING bencher: Skip Megatron-LM (Comp-FP32): Initialization failed with shape (4, 16, 512, 512)\n",
      "[2022-10-28 20:15:37] WARNING bencher: Skip Megatron-LM (Comp-FP32): Initialization failed with shape (8, 16, 512, 512)\n",
      "[---------------------------------- Softmax with FP16 input -----------------------------------]\n",
      "                         |  PyTorch (Comp-FP32)  |  PyTorch (Comp-FP16)  |  xFormers (Comp-FP32)\n",
      "1 threads: -------------------------------------------------------------------------------------\n",
      "      (4, 16, 512, 512)  |         1215.1        |         380.2         |         617.8        \n",
      "      (8, 16, 512, 512)  |         2364.9        |         689.9         |         735.4        \n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "      Shape         PyTorch (Comp-FP32)    PyTorch (Comp-FP16)    xFormers (Comp-FP32)\n",
      "-----------------  ---------------------  ---------------------  ----------------------\n",
      "(4, 16, 512, 512)           288                    96                      64\n",
      "(8, 16, 512, 512)           576                    192                    128\n",
      "\n",
      "Memory is in MBs and excludes inputs/outputs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 -m epoi.benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34448faa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-bench-env",
   "language": "python",
   "name": "hf-bench-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
