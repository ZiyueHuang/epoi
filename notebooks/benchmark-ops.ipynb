{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a120bbad",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "This notebook shows the benchmark results of all covered ops.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de4e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def build_package(name, url, commit=None, deps=True):\n",
    "    import importlib\n",
    "    import os, sys\n",
    "    if importlib.util.find_spec(name) is None:\n",
    "        os.system(f\"git clone {url} {name} || true\")\n",
    "        if commit is not None:\n",
    "            os.system(f\"cd {name}; git checkout {commit}\")\n",
    "        os.system(f\"cd {name}; git submodule update --init --recursive\")\n",
    "        no_deps = \"\"\n",
    "        if deps:\n",
    "            os.system(f\"cd {name}; pip3 install -r requirements.txt || true\")\n",
    "        else:\n",
    "            no_deps = \"--no-deps\"\n",
    "        clear_output()\n",
    "        os.system(f'cd {name}; pip3 install -e \".[dev]\" {no_deps}')\n",
    "\n",
    "build_package(\"transformers\", \"https://github.com/huggingface/transformers.git\", deps=False)\n",
    "build_package(\"xformers\", \"https://github.com/facebookresearch/xformers.git\")\n",
    "build_package(\"epoi\", \"https://github.com/comaniac/epoi.git\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606041c3",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df76cc65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoscrolling long output is disabled\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Javascript\n",
    "\n",
    "disable_js = \"\"\"\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def load_ipython_extension():\n",
    "    display(Javascript(disable_js))\n",
    "    print (\"autoscrolling long output is disabled\")\n",
    "    \n",
    "load_ipython_extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd41d37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Environment =====\n",
      "\n",
      "GPU: Tesla V100-SXM2-16GB\n",
      "\n",
      "PyTorch Configuration\n",
      "   Config         Value\n",
      "-------------  ------------\n",
      "   Version     1.13.0+cu117\n",
      "Built w. CUDA      11.7\n",
      "\n",
      "\n",
      "Other Libraries Configuration\n",
      "  Package       Version                   Commit SHA\n",
      "------------  -----------  ----------------------------------------\n",
      "    epoi        0.1.dev    1dec28727c0f842b6083f6c7b3b6a598d3e330aa\n",
      "transformers  4.25.0.dev0  2bdd9fa28411a2822cd1395ed78abeef4a69ec6f\n",
      "  xformers    0.0.14.dev   ba93c5012d00bd1b010514a7bc9bd938c1ad6149\n",
      "  megatron        N/A      0bb597b42c53355a567aba2a1357cc34b9d99ddd\n",
      "   triton        2.0.0                       N/A\n",
      "    apex          0.1                        N/A\n",
      "===== Environment =====\n",
      "\n",
      "[2022-11-10 00:26:30] INFO main: Selected bias_gelu\n",
      "[2022-11-10 00:26:30] INFO main: Selected dropout_add_ln\n",
      "[2022-11-10 00:26:30] INFO main: Selected bert_attention\n",
      "[2022-11-10 00:26:30] INFO main: Selected gpt_attention\n",
      "[2022-11-10 00:26:30] INFO main: Selected qkv_self_attn\n",
      "[2022-11-10 00:26:30] INFO main: Selected t5_attention\n",
      "[2022-11-10 00:26:30] INFO main: Selected layer_norm\n",
      "[2022-11-10 00:26:30] INFO main: Selected softmax\n",
      "[2022-11-10 00:26:30] INFO main: Running selected 8/8 cases\n",
      "[2022-11-10 00:26:30] INFO main: [1/8] Benchmarking bias_gelu\n",
      "[------------------------------------------- Bias+GeLU --------------------------------------------]\n",
      "                        |  Eager (FP32)  |  TS+nvFuser (FP32)  |  Eager (FP16)  |  TS+nvFuser (FP16)\n",
      "1 threads: -----------------------------------------------------------------------------------------\n",
      "      (8, 512, 1024)    |      232.2     |         326.1       |      201.7     |         328.9     \n",
      "      (8, 512, 768)     |      191.2     |         319.1       |      193.2     |         328.2     \n",
      "      (2, 1024, 4096)   |      386.6     |         383.8       |      235.4     |         326.2     \n",
      "      (16, 512, 8192)   |     2630.3     |        2035.5       |     1416.9     |        1090.9     \n",
      "      (16, 512, 32768)  |    10278.2     |        7830.6       |     5447.4     |        3967.4     \n",
      "      (4, 2048, 8192)   |     2629.0     |        2031.5       |     1415.5     |        1078.5     \n",
      "      (4, 2048, 32768)  |    10266.8     |        7820.5       |     5439.5     |        3968.2     \n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "     Shape         Eager (FP32)    TS+nvFuser (FP32)    Eager (FP16)    TS+nvFuser (FP16)\n",
      "----------------  --------------  -------------------  --------------  -------------------\n",
      " (8, 512, 1024)      64.0044            64.0044           48.0024            48.0024\n",
      " (8, 512, 768)       48.0034            48.0034            36.002            36.002\n",
      "(2, 1024, 4096)      128.016            128.016           96.0083            96.0083\n",
      "(16, 512, 8192)        768              592.032             384              336.016\n",
      "(16, 512, 32768)       3072             2176.13             1536             1152.06\n",
      "(4, 2048, 8192)        768              592.032             384              336.016\n",
      "(4, 2048, 32768)       3072             2176.13             1536             1152.06\n",
      "\n",
      "Memory is in MBs and excludes inputs/outputs.\n",
      "\n",
      "[2022-11-10 00:27:13] INFO main: [2/8] Benchmarking dropout_add_ln\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/hf-bench-env/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/ubuntu/anaconda3/envs/hf-bench-env/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ubuntu/workspace_hf/bench_pt_op/epoi/benchmark/__main__.py\", line 112, in <module>\n",
      "    main()\n",
      "  File \"/home/ubuntu/workspace_hf/bench_pt_op/epoi/benchmark/__main__.py\", line 105, in main\n",
      "    func(args)\n",
      "  File \"/home/ubuntu/workspace_hf/bench_pt_op/epoi/benchmark/fused_ops.py\", line 39, in dropout_add_ln\n",
      "    bench(\n",
      "  File \"/home/ubuntu/workspace_hf/bench_pt_op/epoi/benchmark/bencher.py\", line 167, in bench\n",
      "    perf_results.append(bencher.timeit(500))\n",
      "  File \"/home/ubuntu/anaconda3/envs/hf-bench-env/lib/python3.8/site-packages/torch/utils/benchmark/utils/timer.py\", line 270, in timeit\n",
      "    raw_times=[self._timeit(number=number)],\n",
      "  File \"/home/ubuntu/anaconda3/envs/hf-bench-env/lib/python3.8/site-packages/torch/utils/benchmark/utils/timer.py\", line 256, in _timeit\n",
      "    return max(self._timer.timeit(number), 1e-9)\n",
      "  File \"/home/ubuntu/anaconda3/envs/hf-bench-env/lib/python3.8/timeit.py\", line 177, in timeit\n",
      "    timing = self.inner(it, self.timer)\n",
      "  File \"<timeit-src>\", line 6, in inner\n",
      "  File \"/home/ubuntu/workspace_hf/bench_pt_op/epoi/benchmark/bencher.py\", line 85, in _forward_backward\n",
      "    torch.cuda.synchronize()\n",
      "  File \"/home/ubuntu/anaconda3/envs/hf-bench-env/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 566, in synchronize\n",
      "    return torch._C._cuda_synchronize()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 -m epoi.benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34448faa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-bench-env",
   "language": "python",
   "name": "hf-bench-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
